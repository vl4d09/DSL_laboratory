from enum import Enum
import re

class TokenType(Enum):
    INTEGER = 'INTEGER'
    PLUS = 'PLUS'
    MINUS = 'MINUS'
    TIMES = 'TIMES'
    DIVIDE = 'DIVIDE'
    LPAREN = 'LPAREN'
    RPAREN = 'RPAREN'
    EOF = 'EOF'

class Token:
    def __init__(self, type, value):
        self.type = type
        self.value = value

    def __str__(self):
        return f'Token({self.type.name}, {repr(self.value)})'

class Lexer:
    def __init__(self, text):
        self.text = text
        self.pos = 0
        self.current_token = None
        self.tokens = []

    def error(self):
        raise Exception('Invalid character')

    def tokenize(self):
        token_specification = [
            (TokenType.INTEGER, r'\d+'),
            (TokenType.PLUS, r'\+'),
            (TokenType.MINUS, r'\-'),
            (TokenType.TIMES, r'\*'),
            (TokenType.DIVIDE, r'\/'),
            (TokenType.LPAREN, r'\('),
            (TokenType.RPAREN, r'\)'),
            (TokenType.EOF, r'\Z')
        ]

        token_regex = '|'.join(f'(?P<{tok.name}>{pattern})' for tok, pattern in token_specification)
        for mo in re.finditer(token_regex, self.text):
            kind = mo.lastgroup
            value = mo.group()
            tok_type = TokenType[kind]
            if tok_type == TokenType.INTEGER:
                value = int(value)  # Convert to integer
            self.tokens.append(Token(tok_type, value))

        self.tokens.append(Token(TokenType.EOF, None))

class AST:
    pass

class BinOp(AST):
    def __init__(self, left, op, right):
        self.left = left
        self.op = op
        self.right = right

class Num(AST):
    def __init__(self, token):
        self.token = token
        self.value = token.value

def print_ast(node, level=0):
    indent = '  ' * level
    if isinstance(node, BinOp):
        print(f'{indent}BinOp:')
        print(f'{indent}  Left:')
        print_ast(node.left, level+2)
        print(f'{indent}  Op: {node.op.value}')
        print(f'{indent}  Right:')
        print_ast(node.right, level+2)
    elif isinstance(node, Num):
        print(f'{indent}Num: {node.value}')

class Parser:
    def __init__(self, lexer):
        self.lexer = lexer
        self.tokens = lexer.tokens
        self.current_token = None
        self.pos = -1
        self.advance()

    def advance(self):
        self.pos += 1
        if self.pos < len(self.tokens):
            self.current_token = self.tokens[self.pos]
        else:
            self.current_token = Token(TokenType.EOF, None)

    def error(self):
        raise Exception('Invalid syntax')

    def parse(self):
        return self.expression()

    def expression(self):
        """ Parse an expression. """
        return self.addition()

    def addition(self):
        """ Handle addition and subtraction. """
        result = self.term()

        while self.current_token.type in (TokenType.PLUS, TokenType.MINUS):
            op = self.current_token
            self.advance()
            right = self.term()
            result = BinOp(left=result, op=op, right=right)

        return result

    def term(self):
        """ Handle multiplication and division. """
        result = self.factor()

        while self.current_token.type in (TokenType.TIMES, TokenType.DIVIDE):
            op = self.current_token
            self.advance()
            right = self.factor()
            result = BinOp(left=result, op=op, right=right)

        return result

    def factor(self):
        """ Handle parentheses and numbers. """
        token = self.current_token

        if token.type == TokenType.INTEGER:
            self.advance()
            return Num(token)
        elif token.type == TokenType.LPAREN:
            self.advance()
            result = self.expression()
            if self.current_token.type != TokenType.RPAREN:
                self.error()
            self.advance()
            return result
        else:
            self.error()

def main():
    # Input arithmetic expression with new operators and parentheses
    text = "3 + 4 * (2 - 1)"

    # Create a lexer and tokenize the input text
    lexer = Lexer(text)
    lexer.tokenize()

    # Create a parser and parse the tokens into an AST
    parser = Parser(lexer)
    ast = parser.parse()

    # Print tokens generated by lexer
    print("Tokens:")
    for token in lexer.tokens:
        print(token)

    # Print AST generated by parser
    print("\nAST:")
    print_ast(ast)

if __name__ == "__main__":
    main()

